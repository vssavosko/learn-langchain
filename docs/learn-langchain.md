# Learn LangChain.js

## Введение в LangChain.js

**LangChain.js** — это современный фреймворк для создания приложений на базе больших языковых моделей (LLM), с акцентом на работу с контекстом, знанием и логическим выводом. Автор — Jacob Lee, ведущий мейнтейнер LangChain.js.

### История и мотивация

- **Зачем создан LangChain**: Разработка сложных AI-приложений часто требует одних и тех же абстракций (цепочки вызовов, доступ к данным, пре- и постобработка ввода/вывода). LangChain выделяет эти абстракции в отдельный инструмент, чтобы ускорить создание LLM-решений.
- **Две экосистемы**: Python — лидер AI-разработки, но веб-сообщество нуждается в похожих инструментах для JavaScript. LangChain доступен для обеих платформ (Python и JS).
- **Миссия LangChain.js**: Сделать LLM и их интеграцию доступными разработчикам фронтенда, веба и Node.js.

## Основные концепции LangChain.js

### Ключевые идеи

- **Контекстуальные AI-приложения**: Умение извлечь новую информацию из документов и поддерживать осмысленный диалог по этим данным.
- **Модульная архитектура**: Легкая смена компонентов (векторы, базы данных, API больших языковых моделей).

### Базовые строительные блоки

- **Модели (Models):** LLM, например, GPT через OpenAI API.
- **Промпты (Prompts):** Шаблоны для управления тем, как формулируются запросы к модели.
- **Парсеры (Parsers):** Обработка и структурирование выходных данных модели.
- **Цепочки (Chains):** Позволяют создавать последовательности вызовов, где результат одного шага становится входом для следующего этапа.

## Практическое применение: Retrieval-Augmented Generation

### Концепция

- **Документно-ориентированный retrieval**: Модель получает доступ к специфической информации из ваших документов, что позволяет отвечать на вопросы и вести диалог по свежим данным, недоступным ей на момент обучения.

- _Пример случая_:
  - Модель отвечает на вопросы о содержании документа, который был загружен после её обучения.
  - Возможно построение интерактивного чат-бота для поддержки пользователей: модель ссылается на внутреннюю документацию компании.

### Компоненты и интеграция

- **База данных**: Для хранения и поиска документов используют различные векторные базы данных (например, Supabase).
- **LLM API**: OpenAI или другие поставщики.
- **Гибкость**: Можно легко сменить базу данных или LLM; архитектура позволяет использовать любые совместимые сервисы.

## Пример архитектуры приложения

1. **Загрузка документа**
2. **Индексация документа в векторной базе**
3. **Взаимодействие с пользователем через чат или интерфейс**
4. **Запрос к документу → обработка → ответ через LLM**
5. **Возможность расширять pipeline новыми этапами**

## Краткое резюме и основные идеи

- **LangChain.js** — гибкий и мощный фреймворк для работы с LLM через JS/TS.
- Позволяет создавать приложения, которые не просто используют языковую модель, но умеют взаимодействовать с актуальными документами и данными.
- Сильная сторона — модульная архитектура и способность интегрировать различные базы и модели для поиска и диалога.
- В курсе рассматривается создание retrieval-ориентированных (RAG) приложений на базе Supabase и OpenAI.
- Абстракции LangChain.js (модели, промпты, цепочки) ускоряют и упрощают процесс разработки AI-продуктов.

## Зачем разбивать текст для чат-бота

В этом уроке объясняется, почему важно разбивать исходный документ на части, если вы хотите создать качественного retrieval-чатбота на базе LangChain.js.

### Основная идея

Исходный текст, который будет служить источником знаний для чатбота, нужно делить на логические **чанки** (фрагменты, куски). Каждый чанк должен содержать достаточно информации, чтобы по нему можно было дать осмысленный и полный ответ на потенциальный вопрос. При этом нежелательно, чтобы части разрезали смысловую мысль надвое — например, начало ответа оказалось в одном чанке, а конец в другом.

### Оптимальный размер чанка

Чанк обычно представляет собой абзац или даже несколько связанных абзацев. Совершенно нормально, если в одном чанке содержится сразу несколько смысловых единиц или фактов: цель разбивки — дать языковой модели максимально удобные по размеру сегменты текста, в которых скорее всего найдётся нужный ответ, но которые не избыточны по объёму.

### Почему нельзя просто использовать целый документ

Отправлять каждый пользовательский запрос вместе со всем большим текстом (например, файлом на 3000 слов) напрямую в OpenAI API очень неэффективно:

- существенно увеличиваются расходы на токены;
- падает производительность;
- система плохо масштабируется с ростом объемов данных.

Разбиение на чанки помогает искать релевантную часть текста, ограничивая передаваемый языковой модели контекст только наиболее полезными и нужными для ответа фрагментами.

### Проверка качества исходных данных

Важно, чтобы исходный текст был тщательно отобран и не содержал недостоверной или ошибочной информации. Качество базы знаний напрямую влияет на final-ответы чатбота.

### Использование разных форматов документов с LangChain

В примере из урока используется обычный текстовый файл для простоты, но LangChain поддерживает различные форматы документов:

- PDF,
- текст, извлечённый из HTML,
- и другие источники.

В документации LangChain описаны методы подключения и работы с каждым из вполне типовых форматов, что удобно для разработчика.

### Пример получения текста

В файле index.js показан пример загрузки текста с помощью fetch, обработки ошибок с try/catch и сохранения содержимого для дальнейших операций.

В следующем шаге разрабатывается использование специального инструмента — text splitter — из пакета langchain для непосредственного разбиения текста на чанки.

## Краткое резюме и основные идеи

- Для retrieval-чатбота важно делить исходный документ на логические чанки.
- Каждый чанк должен быть информативным, неделить смысловые идеи на части и содержать достаточно информации для ответа.
- Использование небольшой части документа вместо полного текста уменьшает расходы на токены и ускоряет работу.
- Исходная база знаний должна быть тщательно проверена на корректность.
- LangChain поддерживает разные форматы документов; в уроке разбирается работа с обычным текстом.
- Следующий этап: непосредственное разбиение текста с помощью специализированного инструмента LangChain.

## Инструменты и процесс разбиения текста в LangChain.js

В этом уроке подробно рассматривается практическая реализация разбиения текста на чанки с помощью инструментов LangChain.js, а также принципы, которые лежат в основе процесса.

### Как использовать text splitter в LangChain.js

Главная задача — взять исходный текст и с помощью специального компонента LangChain определить, как его поделить на куски в зависимости от длины и структуры.

LangChain предлагает два основных инструмента для разбиения:

- `CharacterTextSplitter` — простой разделитель по символам.
- `RecursiveCharacterTextSplitter` — более интеллектуальный инструмент, который лучше сохраняет смысловые структуры текста.

В большинстве случаев предпочтительнее использовать recursive splitter, поскольку он старается не разбивать предложения, абзацы и целые смысловые блоки.

### Подключение зависимостей и базовый код

В Scrimba зависимость langchain устанавливается через встроенное меню, а вне платформы производится через npm install langchain.

В коде импортируется RecursiveCharacterTextSplitter. Создаётся его экземпляр, затем вызывается метод createDocuments, который принимает массив текстов и возвращает массив объектов, каждый из которых представляет чанк текста с дополнительными метаданными (например, номерами строк в исходнике).

Важно помнить, что разбиение — асинхронная операция, поэтому её результат нужно await-ить.

Вывод чанков должен происходить в консоль или отдельный файл для дальнейшей работы.

### Размер чанка: как выбирать

Стандартный размер чанка по умолчанию — 1000 символов. Автор урока отмечает, что оптимальный размер варьируется в зависимости от структуры текста, целей и размера базы знаний. В конкретном примере размер уменьшён до 500 символов, чтобы чанки лучше соответствовали смысловым блокам текста и не были избыточно крупными.

Чем больше чанк — тем больше контекста и информации внутри, однако становится труднее гарантировать точность поиска релевантных фрагментов и экономно расходовать токены языковой модели.

### Перекрытие чанков (chunk overlap)

Перекрытие чанков — это повторяющаяся часть текста в соседних чанках. По умолчанию overlap равен 200 символам (для чанка в 1000 символов), но рекомендуется установить значение в районе 10% от размера чанка (в примере выбрано 50 символов). Перекрытие помогает избежать потери важной информации на границах чанков.

Однако overlap не всегда применяется: если разбиение произошло на логическом разделителе (двойной или одиночный перенос строки), то соседние чанки не будут перекрывать друг друга.

### Система разделителей

RecursiveCharacterTextSplitter использует последовательность разделителей: двойной перенос строки, одиночный перенос, пробел, отсутствие пробела. Это позволяет разбивать текст максимально осмысленно. Благодаря этим разделителям, результат разбиения соответствует структуре исходного документа, сохраняются предложения и абзацы.

Можно добавлять свои разделители — например, строки с двойным хэштегом или другие маркеры глава/раздел.

### Итоги работы с чанками

После разбиения текста получается массив объектов, где каждый содержит поле с текстом чанка, а также метаданные (например, номера строк в исходном документе). Это бывает полезно для отладки или если в будущем потребуется ссылаться на конкретные фрагменты документа.

Далее эти чанки готовы к загрузке в векторное хранилище и дальнейшей работе retrieval-бота.

## Краткое резюме и основные идеи

- В LangChain.js для разбиения текста применяются CharacterTextSplitter и RecursiveCharacterTextSplitter.
- Рекомендуется использовать recursive splitter: он учитывает структуру документа и сохраняет целостность смысловых блоков.
- Размер чанка и overlap можно настраивать; оптимальные значения зависят от задач.
- Система разделителей позволяет гибко управлять процессом разбиения и подстраиваться под любой формат текста.
- После разбиения получается структурированный массив чанков с метаданными, удобный для загрузки в векторную базу данных.
- Экспериментируйте с параметрами разбиения для лучшего результата с вашим типом документа и задачами чатбота.

## Загрузка чанков документа в Supabase через LangChain.js

В этом материале подробно разбирается процесс интеграции LangChain.js с Supabase для хранения и поиска чанков текста на основе их эмбеддингов. Приведены этапы настройки окружения, подключения зависимостей и взаимодействия с API для успешной загрузки данных в векторное хранилище Supabase.

### Зачем нужен Supabase и Vector Store

Supabase — это современная облачная платформа для работы с базами данных, а Vector Store — способ хранения векторных представлений (эмбеддингов) текстовых данных, что значительно ускоряет поиск релевантных фрагментов документа для retrieval-чатботов и других LLM-приложений.

Векторное хранилище позволяет быстро сопоставлять пользовательский запрос с ближайшими по смыслу чанками исходного текста на основе их эмбеддингов.

### Создание эмбеддингов и загрузка в Vector Store

Основные шаги работы:

1. Импортируются классы SupabaseVectorStore (из Longchain) и OpenAIEmbeddings.
2. Вся коллекция чанков, полученных на прошлом этапе разбиения текста, подаётся на вход методу fromDocuments класса Vector Store.
3. Помимо массива чанков, сюда передаются:
   - объект Embeddings (с OpenAI ключом),
   - объект с деталями Supabase: клиент и название таблицы (например, documents).

Пример:

```js
-SupabaseVectorStore.fromDocuments(
  [...documents],
  new OpenAIEmbeddings({ openAIApiKey }),
  { client, tableName: "documents" }
);
```

Главное — указывать правильное имя таблицы (tableName), оно должно соответствовать реальной таблице Supabase, куда будут записываться данные.

### Проверка успеха

После выполнения загрузки рекомендуется проверить результат через дашборд Supabase, перейдя к соответствующей таблице (например, documents). Загрузка считается успешной, если видны строки с полями:

- id (уникальный идентификатор записи)
- content (текст чанка)
- metadata (дополнительные данные)
- embedding (вектор эмбеддинга, обычно массив длиной 1536)

Рабочие данные уже доступны для поиска по векторному пространству.

### Важные моменты и детали

- Размер поля embedding может быть очень большим — векторы хранят по 1536 (или больше) значений для одного текста.
- Таблица Supabase должна быть правильно настроена для хранения вложенных массивов/структур.
- Все операции асинхронные — рекомендуется использовать await и отслеживать потенциальные ошибки.

### Следующий этап

После загрузки данных в Supabase можно приступать к реализации поиска по векторному хранилищу, чтобы построить полноценное retrieval-приложение.

## Краткое резюме и основные идеи

- Supabase используется как облачное векторное хранилище для текстовых чанков.
- Эмбеддинги генерируются через OpenAI API, а сами чанки хранятся вместе с метаданными.
- Весь процесс автоматизирован через классы LangChain.js; SupabaseVectorStore и OpenAIEmbeddings.
- Проверить результат загрузки можно в интерфейсе Supabase; все данные видны в выбранной таблице.
- После успешной загрузки можно реализовать быстрый поиск ответов по смыслу через векторные запросы.
- Хранение эмбеддингов и интеграция Supabase расширяют возможности retrieval-ботов и любых AI-приложений, работающих с текстом.

## Объяснение концепции Standalone Question в LangChain.js

### Введение

В этом модуле подробно рассматривается важное понятие — **standalone question** (автономный вопрос) для чатботов и систем на базе LangChain.js. Работа с вопросами пользователей напрямую влияет на качество поиска релевантных ответов и обрабатывается с помощью векторных баз данных.

### Ключевые задачи при обработке пользовательского запроса

При получении входных данных от пользователя, система обычно выполняет два действия:

- **Формирует standalone question** — то есть перефразирует исходный вопрос так, чтобы он содержал только суть запроса.
- **Добавляет исходный ввод пользователя в память диалога** — используется в конце обработки.

В этом объяснении основное внимание уделяется первому этапу.

## Что такое Standalone Question?

**Standalone question** — это вопрос, сведённый к минимально необходимым словам, которые максимально ясно выражают запрос информации.

#### Зачем это нужно?

- Пользователи могут формулировать вопросы слишком длинно или неструктурированно.
- Избыточные, эмоциональные или отвлечённые фразы затрудняют выдачу точного ответа, потому что «семантический вектор» такого запроса будет «зашумлен» лишними деталями.
- Для чатбота важно понять истинное намерение пользователя, извлечь смысл вопроса, чтобы эффективно искать ответ в знаниевой базе.

### Пример кейса

**Сценарий:**
Пользователь пишет:  
_"Я думаю купить у вас футболку, но хотел бы узнать, как работает возврат товара, потому что иногда футболки мне не подходят и не хочется терять деньги."_

**Анализ:**

- Это нормальный человеческий вопрос, но в нём много контекста, который уводит от ключевой сути.
- Для поиска ответа в документации нужно понять основное намерение: что пользователь хочет узнать?

**Результат формирования standalone question:**

- Из длинного запроса выделяется смысл:  
  _"Мне нужно узнать вашу политику возврата товара."_

- Финальный автономный вопрос, который используется для поиска:  
  **"Какая у вас политика возврата?"**

### Технический смысл и использование в LangChain

- Чатбот ищет ответ по принципу ближайшей семантической схожести между вопросом пользователя и кусками документов (embedding + vector store).
- Векторные представления рассчитаны на поиск по смыслу, поэтому чем короче и точнее standalone question, тем выше вероятность получить релевантный фрагмент документа для ответа.

### Связь с памятью диалога

_Обработка памяти диалога (добавление запроса пользователя) обсуждается отдельно, на следующем этапе курса._

## Краткое резюме и основные идеи

- **Standalone question** — это максимально лаконичная формулировка запроса, отражающая суть желания пользователя.
- Преобразование «разговорного» вопроса в standalone question критично для повышения точности и релевантности поиска ответа в базе знаний.
- Векторное сравнение проводится на основе смыслового содержания, лишние детали ухудшают результаты.
- Пример: длинные вопросы о покупке и возврате преобразуются в короткий standalone question: _«Какая у вас политика возврата?»_
- Эффективная обработка запросов на стадии standalone question — ключевой шаг при реализации качественного чатбота на основе LangChain.js.

## Введение в Prompt Templates в LangChain.js

В этом разделе дается подробное объяснение концепции шаблонов подсказок (prompt templates) в LangChain.js. Рассматриваются основные задачи создания шаблонов, технические детали и способы применения для генерации контента.

### Общая задача

Модуль показывает, как перейти от основной задачи диалогового ассистента к созданию приложения, предназначенного для генерации промо-текста для продукта. Акцент делается на демонстрации использования шаблонов подсказок для автоматического составления, например, рекламных твитов на основе описания продукта.

## Основные компоненты

### Зависимости и инициализация

- Используется библиотека `langchain` (версия 0.0.152)
- Подключается OpenAI API key (можно задать вручную или взять из `process.env`)
- Главные классы из LangChain:
  - **ChatOpenAI** — класс для взаимодействия с языковой моделью
  - **PromptTemplate** — для создания и обработки шаблонов подсказок

## Создание простого шаблона

#### Шаги

1. Импорт необходимых классов
2. Настройка экземпляра языковой модели (LLM) через `new ChatOpenAI`, передача API-ключа и, при необходимости, дополнительных параметров (например, изменение температуры)
3. Формирование шаблона текста для запроса:
   - Например, «Сгенерируй промо-твит для продукта на основе его описания: {product_description}»
   - Переменная `{product_description}` задается в фигурных скобках

#### Важно

- Переменные внутри шаблона указываются только в фигурных скобках, без знака доллара, как в обычных шаблонах JS
- LangChain автоматически распознает такие переменные, их можно использовать несколько в одном шаблоне (например, добавить цену, характеристики)

### Инициализация PromptTemplate

Шаблон преобразуется в объект с помощью метода `.fromTemplate()`

- В результате объект получает свойство `inputVariables`, где перечислены все входные переменные шаблона
- Это массив, куда попадут все ожидаемые параметры; если их передать не все, будет выброшена ошибка

### Формат шаблона: f-string

- Формат шаблона аналогичен **f-string** из Python — позволяет удобно вставлять значения переменных в текст подсказки
- Такой формат сделан для легкой и гибкой интеграции с данными в текстовых запросах к модели

## Пример кода

Приведен рабочий пример декларации:

```js
const tweetTemplate =
  "Сгенерируй промо-твит для продукта: {product_description}";
const prompt = PromptTemplate.fromTemplate(tweetTemplate);
```

Можно добавить дополнительные переменные, упростить или усложнить шаблон в зависимости от требований.

### Валидация входных данных

- Если не передать все необходимые переменные при генерации, LangChain выбросит ошибку
- Все входные переменные можно проверить через свойство `inputVariables` в шаблоне

## Краткое резюме и основные идеи

- **Prompt template** — шаблон текстовой подсказки для языковой модели, где переменные заключены в фигурные скобки
- Для создания шаблона используется класс PromptTemplate; переменные автоматически извлекаются и валидируются
- Формат шаблона напоминает f-string из Python, что облегчает интеграцию с языковыми моделями
- Можно задавать любое число входных переменных для гибких сценариев генерации контента
- Незаменим для автоматизации типовых задач: генерация промо-текстов, сообщений, описаний продуктов
- Важно валидировать входные данные и следить за совпадением имен переменных между шаблоном и входом
- Применение prompt templates позволяет быстро масштабировать и кастомизировать работу ассистента или контент-генератора на базе LangChain

## Введение: Добавление первой цепочки (chain) в LangChain.js

Модуль посвящён шагам по созданию первой полноценной цепочки (chain) в LangChain.js и демонстрирует базовый цикл запроса к языковой модели при обработке пользовательских вопросов.

### Основная задача модуля

- Научиться превращать длинный и сложный пользовательский вопрос в структурированный **standalone question** — короткий автономный запрос, пригодный для последующего поиска информации.
- На практике: построить цепочку (chain), которая последовательно применяется к исходному запросу, переформулирует его и возвращает преобразованный вариант для дальнейшей работы.

## Ключевые компоненты и этапы

### 1. Импорт зависимостей и инициализация

- Используется библиотека `langchain` (модульные классы: ChatOpenAI, PromptTemplate).
- Подключается OpenAI API key для работы языковой модели.
- Экземпляр языковой модели создаётся через `new ChatOpenAI({ openAIApiKey })`.

### 2. Формулировка шаблона для перефразирования вопроса

- Создаётся строка-шаблон, которая задаёт задачу преобразования:
  - «Дан вопрос, преобразуй его в standalone question.»
  - В шаблоне используется переменная `{question}`.
  - Шаблон приглашает модель дать краткий автономный вопрос без лишней формулировки.

#### Пример шаблона

question: {question}
standalone question:

### 3. Инициализация PromptTemplate и создание цепочки

- Шаблон превращается в объект PromptTemplate с помощью метода `.fromTemplate`.
- Затем цепочка формируется путём «пайпинга» (pipe) — объединения шаблона и языковой модели.
- Связка выглядит как: prompt.pipe(llm)

### 4. Вызов цепочки

- Цепочка вызывается асинхронно методом `invoke`, с передачей объекта с переменной-
  question.
- В качестве примера подставляется длинный вопрос пользователя с ненужными деталями.
- Возвращается сжатый, уточнённый автономный вопрос.
- Для проверки результат логируется в консоль.

## Пример полного цикла

1. Пользователь задаёт длинный вопрос:  
   «Какие технические требования для запуска Scrimba? У меня очень старый непроизводительный ноутбук.»

2. После работы цепочки результат:  
   «Может ли старый ноутбук соответствовать техническим требованиям для Scrimba?»

3. Итог — цепочка успешно сокращает вопрос до автономной формулировки.

## Организация проекта в модуле

- Не происходит связывания с пользовательским чат-интерфейсом — это отдельный этап, разбираемый далее.
- В модуле созданы все необходимые константы для работы с цепочкой.
- Даны подсказки по синтаксису и порядке действий, чтобы сфокусировать внимание на практике.

## Краткое резюме и основные идеи

- Цепочка (chain) — это механизм последовательной обработки запроса: шаблон → языковая модель → выход.
- Основное задание — грамотно формулировать prompt, чтобы модель могла превратить длинный вопрос в standalone question.
- Метод pipe позволяет связывать шаблон и модель для организации структурного потока данных.
- Все переменные в шаблоне должны быть явно указаны и переданы при вызове chain.
- Применение chain критично для построения эффективных и гибких систем на основе LangChain.js, особенно при сложной обработке пользовательских запросов.
- Создание отдельной цепочки для перефразирования вопроса — первый шаг к построению более сложных диалоговых и поисковых систем.
- Главная идея — минимизация «шума» в запросе и увеличение эффективности поиска релевантных данных в последующих этапах.

## Введение в Retrieval в LangChain.js

В этом разделе подробно рассматривается этап retrieval (извлечение данных) в типовой пайплайне LangChain.js-приложения. Особое внимание уделяется связке с векторным поиском и использованию embedding для генерации и подбора релевантных документов.

### Общий процесс Retrieval

1. Пользовательский запрос преобразуется в standalone question (автономный вопрос).
2. Создаётся embedding запроса — компактное векторное представление смысла текста.
3. Поиск ближайших совпадений (nearest match) осуществляется по векторному пространству среди ранее сохранённых embeddings (документов или знаний).
4. Возвращается один или несколько наиболее релевантных chunks (фрагментов) документации для дальнейшей работы.

## Технологические детали и зависимости

- Применяются модули из LangChain.js:
  - ChatOpenAI — для работы с языковой моделью
  - PromptTemplate — для формирования структуры запросов
  - SupabaseVectorStore — для работы с векторным хранилищем
  - OpenAIEmbeddings — генерирует embedding для новых текстов
- Для хранения и поиска embeddings используется Supabase, который настраивается с помощью API-ключа и URL.

## Работа с embedding и vector store

- На вход векторное хранилище получает embedding standalone question.
- Затем в store осуществляется поиск наиболее близких по смыслу embeddings, что позволяет эффективно решать задачу поиска нужной информации по смыслу, а не по ключевому слову.
- Можно настраивать количество chunk-ов, которые возвращаются: по умолчанию может быть несколько — например, 2, 3 или 10 — в зависимости от сценария и параметров запроса.

## Пример настройки и вызова retrieval

1. Создается экземпляр embeddings с передачей API-ключа:
   OpenAIEmbeddings({ openAIApiKey: '...' })
2. Supabase настраивается с помощью API-ключа и URL, а также задаются имя таблицы и функция match_documents для поиска.
3. Создаётся retriever путем вызова метода asRetriever() у объекта vector store.
4. Теперь retriever можно «прицеплять» к цепочке (chain) LangChain для использования в потоках данных и обработке запросов.

## Вызов и тестирование retriever

- После настройки retriever может быть вызван с standalone question (например, "Подходит ли старый ноутбук для Scrimba?").
- Возвращаются chunks, содержащие релевантные тексты — напр., требования к системе или спецификация платформы.
- Если retriever вызывается отдельно, всё работает корректно; однако при включении retriever в chain иногда может возникать ошибка (типичный пример: ошибка из-за передачи объекта вместо строки).
- Это связано с особенностями типов данных, которые разные элементы цепочки ожидают на входе и выходе.
- Для правильной работы retriever необходимо явно передавать строку standalone question, а не объект, либо использовать output parser (подробнее о нём рассказывается в следующих разделах).

## Типичные проблемы при интеграции

- Самая распространённая ошибка — несоответствие ожидаемого формата данных на входе retriever.
- В chain при передаче outputs одного шага нужно тщательно контролировать, что из данных передается дальше: например, выделять поле value или explicit string.
- LangChain предлагает встроенные механизмы для управления этим — output parser.
- Такой парсер позволяет явно задать, что возвращать из JSON/объекта на каждом шаге, чтобы следующая компонента pipeline получала корректные данные.

## Краткое резюме и основные идеи

- Retrieval — ключевой этап, когда embedding standalone question используется для поиска релевантных chunk-ов в vector store.
- SupabaseVectorStore и OpenAIEmbeddings позволяют реализовать эффективный смысловой поиск по знаниям.
- Retrievers оформляются на основе vector store и интегрируются в цепочки LangChain.js.
- При построении chain важно учитывать формат передачи данных между шагами — часто требуется output parser.
- Корректная работа retrieval определяет релевантность и качество выдачи ответов на пользовательские запросы.
- Все современные RAG-приложения, построенные на LangChain.js, строятся вокруг связки: standalone question → embeddings → retrieval → выбор chunk-ов для ответа.

## Введение и назначение StringOutputParser в LangChain.js

Модуль посвящён внедрению и использованию простого парсера вывода — StringOutputParser — для корректной передачи результата между элементами цепочки LangChain.js, особенно при работе с языковой моделью и retrieval.

## Проблема передачи данных в цепочке

В классической цепочке LangChain.js часто возникают ошибки, связанные с форматом промежуточного результата. Пример — ошибка вида "e.replace is not a function", когда на вход следующего элемента цепочки поступает объект, а ожидается строка. Особенно типично при интеграции retriever, который ожидает входной запрос в виде именно строки standalone question.

## Определение и роль Output Parser

**Output parser** — это компонент LangChain.js, отвечающий за интерпретацию и преобразование результата работы модели (или любого шага цепочки) в требуемый формат данных на вход следующего шага.

- Существуют разные типы парсеров: для преобразования в JSON, для бинарных данных и др.
- StringOutputParser — самый простой вариант, возвращающий «чистую строку» без дополнительной упаковки.

## Шаги добавления StringOutputParser в цепочку

1. Импортировать класс StringOutputParser из LangChain.
2. После шага вызова языковой модели (LLM) добавить парсер через метод pipe.
3. Итоговая связка: prompt.pipe(llm).pipe(new StringOutputParser()).pipe(retriever)

## Почему это важно?

- Без парсера результат работы LLM может быть объектом, а не строкой, что вызывает ошибки при передаче данных в retriever или другие элементы.
- С парсером результат всегда приведён к строке — retriever корректно получает standalone question либо другой требуемый текст.

## Демонстрация на практике

- После добавления StringOutputParser к цепочке всё начинает работать: в консоль логируются корректный результат и возвращаются нужные chunks из векторного хранилища.
- Можно временно убрать retriever из цепочки и проверить — результат будет чистой строкой (ответ LLM).
- Без парсера на выходе цепочки получаем объект, что неудобно для дальнейшей обработки.

## Размышления о структуре и рефакторинге

- При большом количестве pipe-операций появляется вопрос читабельности и масштабируемости цепочки.
- Важно понимать отличие между цепочками: отдельно standalone question chain и основной retrieval chain.
- Следующим шагом предлагается сделать рефакторинг и выделить части pipeline по логическим этапам.

## Краткое резюме и основные идеи

- StringOutputParser — базовый парсер вывода, необходимый для корректной передачи результата между этапами LangChain.js chain.
- Решает проблему несоответствия формата данных (объект вместо строки), особенно критично для конструкций prompt ↔ LLM ↔ retriever.
- Значительно упрощает интеграцию retrieval, устраняя ошибки во время передачи standalone question.
- Хорошей практикой является добавление output parser на каждом логическом этапе цепочки, чтобы результат всегда соответствовал ожиданиям следующего шага.
- В дальнейшем рекомендуется выделять отдельные цепочки по этапам (refactoring), чтобы максимально повысить читаемость и масштабируемость кода.

## Концепция шаблона для получения финального ответа в LangChain.js

В этом фрагменте курса подробно разбирается задача получения осмысленного ответа на вопрос пользователя с помощью шаблона prompt для LLM. Акцент делается на интеграции извлечённых chunk-ов (контекста) из векторного хранилища с оригинальным вопросом пользователя, чтобы формировать действительно релевантный и дружелюбный ответ.

## Этапы получения ответа

### 1. Формирование пайплайна

- После шагов по созданию standalone question и поиска релевантных chunk-ов в vector store необходимо объединить эти данные для генерации финального ответа.
- Для этого используется шаблон prompt с двумя основными переменными:
  - **context** — это массив или строка с chunk-ами знаний, найденных в vector store
  - **question** — исходный, оригинальный вопрос пользователя

### 2. Причины такого подхода

- Standalone question используется только для поиска chunk-ов, потому что этот формат максимально облегчает семантический поиск.
- Оригинальный вопрос пользователя содержит дополнительные нюансы, эмоции и детали, которые полезно сохранить для генерации финального ответа LLM.
- Это позволяет LLM давать более человечные и дружественные ответы, использовать дополнительные детали (например, «Я нервничаю, подойдёт ли Scrimba?»), а не только краткий, сухой ответ.
- Яркий пример:
  - Standalone question — «Подходит ли Scrimba для новичков?»
  - Оригинальный вопрос — «Я полный новичок, очень волнуюсь, подойдёт ли Scrimba для меня?»
  - Ответ будет содержать не только сам факт, но и поддержку: «Да, Scrimba отлично вам подойдёт, не переживайте, у нас дружное сообщество.»

## Структура шаблона ответа

- В шаблоне явно определяются переменные: context и question.
- Сам prompt формулируется так:
  - «Ты дружелюбный и энтузиастичный support-бот Scrimba. Ответь на вопрос, используя предоставленный контекст. Если ответа нет, честно сообщи об этом и предложи обратиться на почту helpscrimper.com. Не выдумывай информацию и старайся говорить как друг.»

### Пример текста шаблона

Ты — дружелюбный и энтузиастичный support-бот, отвечающий на вопросы о Scrimba на основании предоставленного контекста.  
Постарайся найти ответ в контексте.  
Если не уверен, скажи, что не знаешь и предложи обратиться на почту helpscrimper.com.  
Не выдумывай информации.  
Отвечай как друг.

**Входные переменные:**

- question — оригинальный вопрос пользователя
- context — chunks из vector store

## Практика внедрения

- Шаблон добавляется в chain через PromptTemplate, где переменные запрашиваются из результатов retrieval.
- Важно правильно подать данные: context зачастую приходит в виде массива chunk-ов и нуждается в обработке.
- На этапе интеграции могут возникнуть ошибки — например, если переменная context не передана в ожидаемом формате. Для обработки этого нужен отдельный рефакторинг кода, рассматриваемый далее.

## Краткое резюме и основные идеи

- Финальный ответ генерируется на основе общего шаблона, включающего context и question, а не только standalone question.
- Это подход позволяет давать ответы, максимально понятные и поддерживающие пользователя, усиливая дружелюбие бота.
- Важно правильно интегрировать контекст — chunks из vector store — и оригинальный вопрос для повышения релевантности и человечности ответа.
- Ошибки на этом этапе зачастую связаны с подачей данных в шаблон prompt, требуют внимательной работы над структурой chain и форматом переменных.
- Такой шаблон — базовая часть системы вопрос-ответ на базе LangChain.js и современных RAG-подходов.

## Введение в RunnableSequence в LangChain.js

Модуль знакомит с концепцией RunnableSequence — подходом к построению сложных цепочек вызова моделей и функций в LangChain.js. Используется практический пример из языкового приложения, демонстрируя последовательную обработку текста с помощью нескольких шагов: пунктуация, грамматика и перевод.

## Ключевые задачи и примеры

### Кейсы обработки предложения

- Исходное предложение со множеством ошибок:  
  "i dont liked mondays"
- В примере отмечены ошибки:
  - отсутствие заглавной буквы
  - отсутствие апострофа в "don't"
  - неправильная форма "liked"
  - отсутствие заглавной буквы в "Mondays"

### Шаги пайплайна

1. Первый шаг: корректировка пунктуации и регистра
   - Модель добавляет апостроф, исправляет буквы
   - Получается: "I don't liked Mondays"
2. Второй шаг: исправление грамматических ошибок
   - Исправляется форма глагола: "I don’t like Mondays"
3. Третий шаг: перевод на другой язык, например, французский
   - Итог: "Je n'aime pas les lundis"

## RunnableSequence — определение и возможности

**RunnableSequence** — это механизм для построения цепочек обработки данных, включающих модели и пользовательские функции.

- Позволяет объединить любое количество действий — например, модельные вызовы, парсер-выводы, функции обработки.
- Использует метод from для формирования цепочки из массива шагов.

### Особенности работы

- В RunnableSequence каждый шаг должен возвращать результат формата, ожидаемого следующей функцией.
- Если формат не совпадает (например, ожидается объект, а приходит строка), возникают ошибки.
- Иногда приходится явно формировать объект, например, функция-обертка, которая возвращает нужную структуру: `{punctuated_sentence: "…"}`.

## Логирование и отладка внутри пайплайна

- RunnableSequence позволяет добавлять "узкие функции" для логирования промежуточных результатов:
  - Можно вставить функцию с логом в любое место цепочки, чтобы видеть состояние данных между этапами.
- На практике: выводим консольное сообщение после каждого важного шага для отладки.

## Типичные проблемы и решения

- Ошибки формата на входе следующего шага (например, грамматический prompt ожидает объект, а приходит строка).
- Для обхода проблемы используется функция, формирующая нужный объект из строки между парсером и следующим prompt.
- Такой подход, хоть и не элегантен, позволяет "протянуть" нужные данные по цепочке.

## Архитектурные выводы

- Для работы RunnableSequence требуется минимум два элемента в цепочке.
- Порядок и формат передачи данных между функциями критически важен.
- По мере усложнения пайплайна становится актуальным вопрос рефакторинга — перенос функций, добавление промежуточных парсеров и обработчиков.

## Краткое резюме и основные идеи

- RunnableSequence — универсальный инструмент для построения сложных цепочек обработки данных в LangChain.js.
- Позволяет объединять любые типы действий: изменения текста, модельные вызовы, преобразования, перевод.
- Требует точного контроля формата данных между шагами, иначе возникнут ошибки.
- В цепочку можно вставлять функции для преобразования, логирования, создания объектов.
- Хороший пайплайн позволяет применять модели к сложным задачам на обработку и перевод текста.
- В дальнейшем рекомендуется заниматься рефакторингом цепочек для повышения читаемости и масштабируемости решения.

## Продвинутое построение цепочек с RunnableSequence. Рефакторинг и расширение

В этом разделе продолжается работа с концепцией RunnableSequence. На примере языкового приложения показывается, как грамотно структурировать сложные цепочки обработки текста: от пунктуации и грамматики до перевода. Особое внимание уделяется рефакторингу пайплайна, выделению цепочек на подэтапы и передаче входных параметров.

## Основные шаги и принципы рефакторинга

### Идея модульности

- Каждую логическую операцию — пунктуация, исправление грамматики, перевод — рекомендуется оформить как отдельную цепочку или функцию.
- Это позволяет переиспользовать части пайплайна, тестировать их по отдельности, повышать читабельность кода.

### Пример выделения цепочек

1. **punctuationChain** — отдельная RunnableSequence, осуществляющая постановку знаков препинания.
2. **grammarChain** — самостоятельная цепочка для коррекции грамматических ошибок.
3. **translationChain** — отвечает только за перевод итогового текста.

- Все три цепочки затем объединяются: сначала вызывается punctuationChain, результат передаётся в grammarChain, после чего — в translationChain.
- Каждый этап может быть реализован либо через RunnableSequence, либо через pipe-синтаксис — выбор зависит от личных предпочтений и задач.

## Передача входных параметров по пайплайну

- Ключевая проблема: корректная передача переменных (например, целевого языка для перевода) на каждом этапе цепочки.
- Если template ожидает переменную language, нужно явно прокидывать её по всей цепочке, иначе возникнут ошибки типа "input variable not found".
- LangChain предоставляет специальные инструменты для корректной передачи всех оригинальных данных между этапами (об этом рассказывается далее).

## Рабочий пример пайплайна

- Исходное предложение: "i dont liked mondays"
- Шаги обработки:

  1. punctuationChain → "I don’t liked Mondays"
  2. grammarChain → "I don’t like Mondays"
  3. translationChain → "Je n'aime pas les lundis" (при переводе на французский)

- При вызове всей цепочки через RunnableSequence, можно построить единый поток вызовов, где каждый шаг возвращает результат для следующего.

## Особенности, плюсы и минусы подхода

- Такой рефакторинг упрощает повторное использование отдельных цепочек в других приложениях или сценариях.
- Передача сторонних параметров (например, языка перевода) требует дополнительного внимания и может вызвать ошибки, если не учесть их на этапе проектирования.
- Для некоторых простых цепочек (prompt + LLM + outputParser) pipe-синтаксис выглядит короче и проще, но для сложных потоков RunnableSequence — оптимальный выбор.

## Важные нюансы

- При построении сложных цепочек возрастает актуальность отладки: добавляйте промежуточные логирующие шаги для анализа состояния данных на каждом этапе.
- Корректно структурированный код облегчает масштабирование, добавление новых этапов и изменяемость логику обработки.

## Краткое резюме и основные идеи

- Рекомендуется структурировать сложные цепочки обработки данных через отдельные RunnableSequence для каждого логического этапа (коррекция пунктуации, грамматики, перевод).
- Передача всех входных переменных и данных по цепочке требует предварительной планировки и явной прокидки.
- Pipe-цепочки подходят для простых сценариев, RunnableSequence — для сложных пайплайнов с несколькими этапами.
- Такой подход повышает читаемость кода, облегчает тестирование, повторное использование, отладку и масштабирование проекта.
- LangChain предоставляет инструменты для строгого контроля передачи данных между шагами, что будет рассмотрено в следующем модуле.

## Продвинутые пайплайны: RunnablePassthrough и прокидывание оригинальных данных в LangChain.js

Этот модуль посвящён важному вопросу — как грамотно прокидывать дополнительные входные переменные (например, целевой язык перевода) через всю цепочку сложного пайплайна с помощью специального инструмента RunnablePassthrough.

## Проблема передачи оригинальных входных данных

- В сложных пайплайнах (например, где сначала делается коррекция пунктуации, затем грамматики, а потом перевод), отдельным шагам могут понадобиться переменные, которые были только во входных данных оригинального запроса.
- Пример: в этап перевода надо прокинуть не только исправленное предложение, но и переменную language, которую пользователь передал на самом первом шаге.
- Без специального инструмента (RunnablePassthrough) эти переменные «теряются» после первых шагов пайплайна, вызывая ошибки типа "Missing value for input language".

## Решение: RunnablePassthrough

**RunnablePassthrough** — это механизм, который позволяет на любом этапе пайплайна сохранить оригинальные переменные и донести их через всю цепочку до нужного шага.

### Как это работает

- После каждого важного шага или после всей ветки цепочки к выходным данным добавляется ключ originalInput с помощью RunnablePassthrough.
- В итоге к любому дальнейшему шагу можно обратиться к originalInput и получить, например, language или искомый параметр.

## Принцип применения в пайплайне

1. На каждом логическом этапе (например, после punctuationChain и grammarChain) к выходному объекту добавляется originalInput — экземпляр RunnablePassthrough.
2. При интеграции translationChain, который ожидает переменную language, можно сделать деструктуризацию: взять language из originalInput, который хранит весь изначальный ввод.
3. Таким образом, необходимая переменная всегда будет на нужном этапе.

### Пример

- Вход: { sentence: "i dont liked mondays", language: "French" }
- После punctuationChain → grammarChain → добавление originalInput с RunnablePassthrough
- В translationChain делается:  
  const { originalInput } = input;  
  const { language } = originalInput;

## Отладка и логирование

- Для понимания правильности передачи данных удобно логировать содержимое originalInput на каждом этапе пайплайна.
- Пример в коде: временно вставлен консольный вывод для инспекции originalInput внутри translationChain.

## Рекомендации по форматированию и архитектуре пайплайна

- Массив шагов пайплайна удобнее разбивать по строкам — так легче анализировать и поддерживать код.
- Использование объекта с оригинальными данными — стиль, повышающий масштабируемость и предотвращающий пропажу нужных полей на любом этапе процедуры.

## Практический результат

- После внедрения RunnablePassthrough пайплайн корректно работает: все переменные, включая language, доходят до translationChain, и финальный перевод выполняется без ошибок.
- При удалении/добавлении других шагов сохранность переменных гарантирована.

## Краткое резюме и основные идеи

- В сложных пайплайнах LangChain.js для передачи всех оригинальных параметров во все этапы используется RunnablePassthrough.
- Он создает дополнительное поле (originalInput), внутри которого хранится изначальный объект input.
- На нужном этапе осуществляется деструктуризация: извлекается нужная переменная из originalInput и передаётся туда, где требуется.
- Это предотвращает типичные ошибки при отсутствии входных переменных на отдельных этапах сложных цепочек.
- Такой подход — обязательная практика при проектировании крупных и масштабируемых пайплайнов обработки данных и генерации текста.

## Решение Super Challenge: построение интегрированной цепочки ответа в LangChain.js

В этом финальном модуле представлен стандарт решения «супер челленджа» — создание связного пайплайна из нескольких цепочек для обработки вопроса пользователя с помощью LLM и retrieval, начиная от трансформации запроса до генерации ответов на основе найденных знаний.

## Основные составные части пайплайна

### 1. Разделение пайплайна на отдельные цепочки

- Вся архитектура решения построена на трёх логических цепочках:
  - **Standalone question chain** — принимает исходный вопрос пользователя, преобразует его в автономный вопрос через prompt, LLM и StringOutputParser.
  - **Retriever chain** — получает standalone question, ищет релевантные документы в векторном хранилище (vector store) и возвращает массив найденных chunk-ов.
  - **Answer chain** — генерирует итоговый текст ответа, используя оригинальный вопрос и найденный контекст.

### 2. Реализация pipe-метода

- Более простые цепочки (standalone question, answer) удобно реализовать через цепочку вызовов pipe — это сокращает код и упрощает структуру.
- Для более сложной логики и передачи данных используется RunnableSequence: позволяет создать массив шагов, добавить функции для обработки входных и выходных данных, и прокидывать оригинальный input через RunnablePassthrough.

### 3. Формирование главной цепочки

- Главный pipeline строится как RunnableSequence, объединяющий все этапы.
- Начальный шаг — генерируется standalone question, затем с помощью narrow-функции извлекается строковое значение для retriever.
- После retriever с помощью утилиты combineDocuments извлекается из массива объектов только текст chunk-ов: формируется строка context.

### 4. Прокидывание исходного вопроса и контекста

- Для передачи нужных данных между цепочками используется объект с двумя ключами:

  - standaloneQuestion — результат standalone question chain
  - originalInput — объект, хранящий все изначальные данные пользователя через RunnablePassthrough

- В answer-подсказке на вход подаются:
  - context — строка с найденными chunk-ами
  - original question — взятый из originalInput исходный вопрос пользователя

## Пример кода

```js
pipeline = [
  standaloneQuestionChain,
  { standaloneQuestion, originalInput: RunnablePassthrough },
  retrieverChain,
  { context: retrieverChain, question: originalInput.question },
  answerChain,
];
```

- Используются утилиты narrow-function и combineDocuments для правильной передачи данных на каждом этапе.

## Логика работы и финальный результат

- Пользователь вводит вопрос.
- Пайплайн формирует standalone question, ищет релевантные chunk-и, объединяет их в context.
- LLM генерирует финальный ответ, учитывая оригинальный вопрос для дружелюбного и релевантного тона.
- Если всё реализовано верно, пайплайн работает стабильно, выводит корректный и человекоподобный ответ на основе актуальных знаний.

## Ошибки, сложности и рекомендации

- Частая проблема: ошибки Missing value for input context, если не прокинуть context между шагами, или не учесть оригинальный вопрос.
- Практический совет — на каждом этапе pipeline явно формировать объект с нужными ключами и использовать RunnablePassthrough для передачи дополнительной информации.
- Рекомендуется рефакторить большие цепочки, разбивать на небольшие reusable chains и проверять корректность прокидывания переменных.

## Краткое резюме и основные идеи

- Эффективный пайплайн — это интеграция нескольких независимых цепочек для автономизации вопроса, поиска chunk-ов и генерации итогового ответа.
- Главная цепочка строится как RunnableSequence, где на вход в каждый этап явно прокидываются нужные переменные через объекты и утилиты.
- Для стабильной передачи данных применяется RunnablePassthrough и narrow-функции, а для преобразования chunk-ов в строку — combineDocuments.
- Такой подход обеспечивает корректную генерацию, дружелюбный тон ответа и гарантирует стабильную работу системы.
- Важно структурировать, документировать и тестировать каждую цепочку по отдельности для последующего масштабирования и сопровождения проекта.

## Итоговое подключение памяти (conversation memory) к цепочке LangChain.js

В этом модуле разбирается окончательная интеграция conversation memory (истории диалога) к основной цепочке чата на базе LangChain.js. Подключение памяти позволяет боту запоминать детали из предыдущих сообщений и отвечать на вопросы, опираясь на весь контекст беседы.

## Задача и архитектурная схема

- На финальном этапе чатбот должен использовать conversation history и на этапе генерации standalone question, и при формировании ответа.
- Такой подход улучшает качество автономных вопросов и позволяет боту «вспоминать» детали о пользователе, например имя или темы предыдущих диалогов.
- Conversation history передаётся через всю цепочку, становится одной из переменных, формирующих prompt для моделей и функций.

## Основные этапы подключения памяти

### 1. Передача истории диалога в цепочку

- В коде при вызове основной цепочки чатбота теперь передаётся переменная confhistory вместе с вопросом пользователя.
- Перед передачей history форматируется функцией formatConvHistory для повышения читабельности и структурированности.
- Форматированная история диалога становится доступна на всех стадиях pipeline.

### 2. Использование conversation history в standalone question

- Standalone question template дополняется новым input: confhistory.
- Сам prompt обновляется: добавлена инструкция использовать не только исходный вопрос, но и всю conversation history для более точного формирования автономного запроса.

### 3. Использование памяти в ответной цепочке (answer chain)

- Answer chain теперь тоже получает confhistory как отдельную переменную.
- В шаблоне prompt рекомендуется явно указать, что ответ может использовать conversation history, но источником знания должен оставаться context (chunk-и из базы знаний).
- В prompt для ответа — инструкция искать ответ сначала в context, если не найдено — использовать историю диалога.

## Пример обновлённого шаблона

Standalone question:

Дан history диалога и вопрос пользователя, сформулируй максимально самостоятельный автономный вопрос.

Answer chain:

Ты энтузиастичный support-бот. Ответь на вопрос на основе предоставленного context и истории диалога. Если ответа нет в context, поищи в истории. Не выдумывай информации.

## Тестирование и верификация

- В результате интеграции памяти бот способен отвечать на вопросы, используя детали из предыдущих сообщений.
- Пример: сначала пользователь сообщает боту имя, затем спрашивает «Как меня зовут?» — бот корректно извлекает имя из истории диалога.
- Сообщения о Scrimba, её сообществе, также обрабатываются ботом с использованием общей и индивидуальной информации — что демонстрирует работу pipeline.

## Краткое резюме и основные идеи

- Conversation memory — обязательный компонент современного чатбота на базе LangChain.js, который усиливает персонализацию и качество ответов.
- Память передаётся и используется как input в standalone question chain и answer chain.
- Ее форматирование и прокидывание через pipeline позволяет извлекать детали из всей истории диалога на любом этапе обработки.
- Итог: бот становится «умным» и запоминающим собеседником, способным отвечать на вопросы о пользователе и контексте предыдущих бесед и документов.
- Такой подход — best practice для создания интеллектуальных чатботов, интегрированных с RAG и retrieval-архитектурой.

## Заключение курса по LangChain.js: обзор ключевых этапов и финальные советы

Этот финальный модуль подводит итог всему курсу по LangChain.js, концентрируется на пройденных технических шагах, инструментах и архитектурных подходах, помогая систематизировать материал для будущей самостоятельной работы.

## Основные этапы и инструменты курса

### 1. Работа с документацией и векторным поиском

- С самого начала рассматривалось, как разбивать документы на chunk-и с помощью текстового split-инструмента LangChain.
- Для каждого chunk-а создавался семантический вектор через модель OpenAI Embeddings.
- Итог — появлялась база данных с embedding-векторами, что становилось фундаментом для retrieval и поиска по смыслу.

### 2. Построение templates и prompt-ов

- Изучены методы создания шаблонных prompt-ов, кастомизации запросов для модели.
- Шаблоны интегрировались с языковой моделью через pipe-метод, добавлялись output parser'ы для интерпретации результатов.
- На базе PromptTemplate можно генерировать разнообразные сценарии, определения автономных вопросов, ответы бота и многое другое.

### 3. Реализация цепочек (chains) и пайплайнов

- Простые chains реализовывались как комбинация prompt → LLM → outputParser.
- После освоения базовых pipe-цепочек был переход к более сложным пайплайнам через RunnableSequence — последовательное объединение шагов, моделей и функций.
- В сложных сценариях данные прокидывались через утилиты вроде RunnablePassthrough.

### 4. Интеграция retrieval с векторным хранилищем

- Пользовательское сообщение векторизовалось и искался ближайший по embedding chunk среди документов.
- Этот шаг — основа современных RAG-платформ, где retrieval объединён с генерацией ответа.

### 5. Генерация финального ответа

- Чанки из векторного поиска подавались на вход LLM вместе с оригинальным вопросом пользователя.
- Итоговый ответ строился так, чтобы был релевантным, дружелюбным и естественным по стилю — с поддержкой, персонализацией и ссылкой на источник.

### 6. Архитектурные паттерны для роста и масштабирования

- Пайплайны структурировались через последовательные chains, а для сложных задач выделялись самостоятельные reusable цепочки.
- Внедрялись утилиты narrow-функций (например, combineDocuments) для обработки массива chunk-ов, прокидка оригинальных input-ов для передачи параметров между этапами pipeline.

## Финальные рекомендации для дальнейшей работы

- Курс дал устойчивую платформу для самостоятельной разработки AI-приложений, чатботов и retrieval-систем.
- Рекомендуется экспериментировать с настройками vector store, embedding моделей и шаблонов prompt-ов.
- Можно расширять цепочки, подключать память диалога, создавать персонализированные боты, развивать архитектуру под сложные RAG-решения.
- Важно использовать лучшие практики модульности, тестирования и рефакторинга пайплайнов — это позволит сопровождать и расширять проекты в будущем.

## Краткое резюме и основные идеи

- Все современные AI-приложения строятся вокруг комбинации: обработка документа → embedding → retrieval → генерация ответа.
- LangChain.js даёт гибкие инструменты для работы с цепочками, retrieval, prompt-ами и комплексными пайплайнами.
- Для сложных задач критична модульность, разделение цепочек, прокидка originalInput и контроль формата данных.
- Курс охватывает практические аспекты генерации, retrieval и архитектуры, даёт опору для самостоятельных экспериментов и проектирования собственных решений.
- Участие в комьюнити, постоянное обновление знаний и поддержка обмена опытом — важная часть профессионального роста в направлении AI и LangChain.
